# ADR-003: Six-Level Transform Hierarchy

## Status

**Accepted** - Implemented in `quantiq.transform`

Date: 2024-10-19

## Context

Scientific data processing involves transformations at multiple granularities:

- **Single dataset**: Smooth a viscosity curve
- **Measurement** (multiple datasets): Normalize all datasets in a measurement
- **Measurement set** (replicates): Average across replicate measurements
- **Experiment** (varying conditions): Interpolate all experiments to common grid
- **Experiment set** (full campaign): Batch-process entire experimental campaign

**Problem**: How do we provide a consistent, type-safe API for transforms across all these levels without code duplication?

**Requirements**:
1. Same transform logic works at any hierarchy level
2. Type system prevents invalid operations
3. Metadata correctly propagated through transforms
4. Composable (transforms can build on each other)
5. Minimal boilerplate for new transforms

## Decision

We implement a **six-level transform hierarchy** using abstract base classes and composition:

```
BaseTransform (abstract)
    ├── DatasetTransform           # Transforms single dataset
    ├── MeasurementTransform        # Transforms measurement (multiple datasets)
    ├── MeasurementSetTransform     # Transforms measurement set (replicates)
    ├── ExperimentTransform         # Transforms experiment (varying conditions)
    ├── ExperimentSetTransform      # Transforms experiment set (full campaign)
    └── Pipeline                    # Composes multiple transforms
```

**Key Design**:

```python
# Each transform level has consistent API
class DatasetTransform(ABC):
    @abstractmethod
    def apply_to(self, dataset: Dataset, make_copy: bool = True) -> Dataset:
        """Transform a single dataset."""
        pass

class MeasurementTransform(ABC):
    @abstractmethod
    def apply_to(self, measurement: Measurement, make_copy: bool = True) -> Measurement:
        """Transform a measurement (applies to all contained datasets)."""
        pass

# ... and so on for each level
```

**Composition Pattern**:

Higher-level transforms delegate to lower-level transforms:

```python
class MeasurementTransform:
    def apply_to(self, measurement: Measurement) -> Measurement:
        # Apply dataset transform to each dataset in measurement
        new_datasets = {
            name: self._dataset_transform.apply_to(ds)
            for name, ds in measurement.datasets.items()
        }
        return measurement.copy_with(datasets=new_datasets)
```

## Consequences

### Positive

1. **Consistent API**:
   - Same method name (`apply_to`) across all levels
   - Same pattern: input → transform → output
   - Easy to learn: know one, know all

2. **Type Safety**:
   - mypy validates correct usage
   - Can't apply DatasetTransform to ExperimentSet
   - IDEs provide correct autocomplete

3. **Code Reuse**:
   - Write transform logic once at dataset level
   - Higher levels automatically compose
   - Example: `GaussianSmooth` works at all levels via composition

4. **Flexibility**:
   - Apply same transform at appropriate level
   - Fine-grained control when needed
   - Coarse-grained batch processing when desired

5. **Pipeline Composition**:
   - Mix transforms from different levels
   - Clear semantics for what gets transformed
   - Easy to build complex workflows

### Negative

1. **Conceptual Complexity**:
   - Six transform classes to understand
   - Users must know data hierarchy first
   - Learning curve steeper than single transform type

2. **Boilerplate for New Transforms**:
   - Must implement `apply_to` for each level
   - Even if higher levels just delegate
   - More code than single-level transforms

3. **Performance Overhead**:
   - Multiple function calls through hierarchy
   - Python call overhead (mitigated by JIT)
   - Type checking at each level

4. **Documentation Burden**:
   - Need examples for each level
   - Explain when to use which level
   - API docs more verbose

### Mitigation Strategies

**For Conceptual Complexity**:

- Clear documentation with hierarchy diagram
- Examples start simple (dataset level) then progress
- Helper functions hide complexity:

```python
# Simple API for common cases
quantiq.smooth(data, sigma=2.0)  # Automatically uses right level
```

**For Boilerplate**:

- Base classes provide default implementations
- Most transforms only override dataset level
- Mixins for common patterns:

```python
class GaussianSmooth(DatasetTransform):
    # Only implement dataset transform
    def apply_to(self, dataset: Dataset) -> Dataset:
        ...
    # Higher levels auto-generated by base class
```

**For Performance**:

- JIT compilation eliminates Python overhead
- Lazy evaluation for pipelines
- Structural sharing (JAX) avoids unnecessary copies

## Alternatives Considered

### Alternative 1: Single Transform Type (Dynamic Dispatch)

```python
class Transform:
    def apply_to(self, data):  # Accepts any type
        if isinstance(data, Dataset):
            return self._apply_dataset(data)
        elif isinstance(data, Measurement):
            return self._apply_measurement(data)
        # ... etc
```

**Pros**:
- Single class to understand
- Simpler conceptual model
- Less boilerplate

**Cons**:
- No type safety (mypy can't help)
- Runtime errors instead of compile-time
- `if isinstance` checks everywhere
- Harder to optimize

**Why Rejected**: Loses type safety, Python's strength

### Alternative 2: Function-Based (No Classes)

```python
def smooth(data, sigma=2.0):
    """Smooth any data structure."""
    # Pattern match on type
    match data:
        case Dataset(): ...
        case Measurement(): ...
```

**Pros**:
- Simple functions, no OOP
- Pythonic
- Easy to understand

**Cons**:
- Can't compose easily (no Pipeline)
- Hard to add new transforms (monkey patching?)
- No shared interface
- Configuration becomes tricky

**Why Rejected**: Not composable, hard to extend

### Alternative 3: Visitor Pattern

```python
class Transform(ABC):
    def visit_dataset(self, dataset): ...
    def visit_measurement(self, measurement): ...

class Dataset:
    def accept(self, visitor: Transform):
        return visitor.visit_dataset(self)
```

**Pros**:
- Clean separation of data and operations
- Extensible for new transforms

**Cons**:
- Heavy OOP pattern, not Pythonic
- Requires modifying data classes for new transforms
- More complex than needed

**Why Rejected**: Over-engineered for Python

### Alternative 4: Flat Hierarchy (3 Levels)

```python
# Only: DatasetTransform, CollectionTransform, Pipeline
```

**Pros**:
- Simpler than 6 levels
- Easier to learn
- Less boilerplate

**Cons**:
- "Collection" too vague (MeasurementSet vs Experiment?)
- Loses semantic clarity
- Type safety degraded

**Why Rejected**: Experimental data naturally has 5+ levels

### Alternative 5: Mixins for All Levels

```python
class DatasetTransformMixin:
    """Automatically implements higher levels."""
    def apply_to_measurement(self, m):
        return self._lift_to_measurement(m)
```

**Pros**:
- Write once, works everywhere
- Minimal boilerplate

**Cons**:
- Magic behavior (implicit lifting)
- Harder to customize higher levels
- Confusing when it doesn't work

**Why Rejected**: Too much magic, not explicit enough

## Implementation Notes

### Creating a New Transform

Most transforms only need to implement dataset level:

```python
from quantiq.transform.base import DatasetTransform

class MyTransform(DatasetTransform):
    """My custom transform."""

    def __init__(self, param: float):
        self.param = param

    def apply_to(self, dataset: OneDimensionalDataset, make_copy: bool = True) -> OneDimensionalDataset:
        """Apply transform to dataset."""
        # Your transform logic here
        new_y = self._process(dataset.dependent_variable_data)

        # Return new dataset
        return dataset.copy_with(
            dependent_variable_data=new_y
        )

# Higher levels automatically work!
my_transform = MyTransform(param=1.5)
my_transform.apply_to(dataset)        # ✅ Works
my_transform.apply_to(measurement)     # ✅ Works via composition
my_transform.apply_to(experiment_set)  # ✅ Works via composition
```

### When to Implement Higher Levels

Only override higher levels when you need **different behavior**:

```python
class AverageReplicates(MeasurementSetTransform):
    """Averages across replicate measurements."""

    def apply_to(self, measurement_set: MeasurementSet) -> MeasurementSet:
        # Custom logic for MeasurementSet
        # (Averaging doesn't make sense at dataset level)
        averaged_data = self._average_measurements(measurement_set.measurements)
        return measurement_set.copy_with(measurements=[averaged_data])
```

### Using Pipelines

```python
from quantiq.transform import Pipeline
from quantiq.transform.dataset import GaussianSmooth, Normalize

pipeline = Pipeline([
    GaussianSmooth(sigma=2.0),     # DatasetTransform
    Normalize(method="min-max"),   # DatasetTransform
])

# Apply pipeline at any level
pipeline.apply_to(dataset)         # ✅ Works
pipeline.apply_to(experiment_set)  # ✅ Works (applies to all contained datasets)
```

## Design Rationale

**Why Six Levels?**

Maps to experimental reality:
1. **Dataset**: Single measurement (x, y data)
2. **Measurement**: Multiple datasets from one experimental run
3. **MeasurementSet**: Replicate measurements (same conditions)
4. **Experiment**: Measurements with varying conditions
5. **ExperimentSet**: Full experimental campaign
6. **Pipeline**: Composition of transforms

**Why Not More?**

More levels = more complexity with diminishing returns. Six levels cover 99% of scientific workflows.

**Why Not Fewer?**

Scientific data naturally has this hierarchy. Collapsing levels loses semantic information and type safety.

## Related Decisions

- **ADR-002**: Immutable Datasets (enables functional transform composition)
- **ADR-004**: Metadata Separation (conditions vs details affects transform propagation)

## References

- Gang of Four Design Patterns (Strategy pattern)
- Functional Programming: Map/Filter/Reduce over hierarchical data
- pandas' multi-level indexing and groupby operations
- xarray's dataset/dataarray hierarchy

## Revision History

- 2024-10-19: Initial ADR creation
- Status: Accepted and implemented
