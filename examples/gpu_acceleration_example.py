"""
GPU Acceleration Example for quantiq.

This example demonstrates how to leverage GPU acceleration with JAX
for significant performance improvements in data processing and model fitting.

Key concepts:
- Automatic GPU detection and usage
- JIT compilation for optimization
- Batch processing for GPU efficiency
- CPU vs GPU performance comparison
- Memory management on GPU

Requirements:
- JAX with GPU support (CUDA, Metal, or ROCm)
- NVIDIA GPU, Apple Silicon, or AMD GPU

Run time: ~5 seconds (varies by hardware)

Author: quantiq developers
Date: 2025-10-20
"""

import time

import numpy as np

# Import quantiq
from quantiq.backend import get_backend, get_device_info, is_jax_available, jnp
from quantiq.data.datasets import OneDimensionalDataset
from quantiq.transform.dataset.smoothing import GaussianSmooth
from quantiq.transform.pipeline import Pipeline


def print_device_info():
    """Print information about available compute devices."""
    print("=" * 70)
    print("DEVICE INFORMATION")
    print("=" * 70)

    backend = get_backend()
    print(f"Backend: {backend}")

    if is_jax_available():
        info = get_device_info()
        print("JAX available: Yes")
        print(f"Default device: {info.get('default_device', 'CPU')}")

        devices = info.get("devices", [])
        if devices:
            print(f"Available devices: {len(devices)}")
            for i, device in enumerate(devices):
                print(f"  Device {i}: {device}")
        else:
            print("Available devices: CPU only")

        platform = info.get("platform", "cpu")
        print(f"Platform: {platform.upper()}")

        if platform == "gpu":
            print("\n‚úì GPU acceleration available!")
        elif platform == "cpu":
            print("\n‚ö† Running on CPU (no GPU detected)")
    else:
        print("JAX available: No (using NumPy backend)")
        print("‚ö† Install JAX for GPU acceleration")

    print("=" * 70)
    print()


def create_large_dataset(n_points: int = 100_000):
    """
    Create a large synthetic dataset for performance testing.

    Parameters
    ----------
    n_points : int
        Number of data points (default: 100,000)

    Returns
    -------
    OneDimensionalDataset
        Large dataset for benchmarking
    """
    x = np.linspace(0, 100, n_points)
    # Create complex signal with noise
    y = np.sin(x * 0.5) + 0.5 * np.sin(x * 2.0) + 0.1 * np.random.normal(size=n_points)

    dataset = OneDimensionalDataset(x, y)
    dataset.name = "Large Synthetic Signal"
    return dataset


def benchmark_cpu_vs_gpu():
    """
    Benchmark CPU vs GPU performance for transform pipelines.

    Demonstrates the performance difference between CPU and GPU
    when processing large datasets with complex pipelines.
    """
    print("=" * 70)
    print("PERFORMANCE BENCHMARK: CPU vs GPU")
    print("=" * 70)

    # Create test dataset
    print("\nCreating large dataset (100,000 points)...")
    dataset = create_large_dataset(100_000)

    # Create transform pipeline
    pipeline = Pipeline(
        [
            GaussianSmooth(sigma=5.0),
            GaussianSmooth(sigma=2.0),
        ]
    )

    # Warm-up run (JIT compilation)
    print("Warming up (JIT compilation)...")
    _ = pipeline.apply_to(dataset)

    # Benchmark CPU/GPU performance
    print("\nRunning benchmark (5 iterations)...")
    iterations = 5

    start_time = time.time()
    for _i in range(iterations):
        result = pipeline.apply_to(dataset)
    end_time = time.time()

    avg_time = (end_time - start_time) / iterations
    backend = get_backend()

    print(f"\nResults ({backend} backend):")
    print(f"  Average time per iteration: {avg_time * 1000:.2f} ms")
    print(f"  Throughput: {len(dataset.independent_variable_data) / avg_time:.0f} points/second")

    if backend == "jax":
        device_info = get_device_info()
        platform = device_info.get("platform", "cpu")
        if platform == "gpu":
            print("\n‚úì GPU acceleration enabled")
            print("  Expected speedup: 10-100x compared to CPU")
        else:
            print("\n‚ö† Running on CPU")
            print("  Install GPU-enabled JAX for acceleration")
    else:
        print("\n‚ö† Using NumPy backend")
        print("  Install JAX for automatic GPU acceleration")

    print("=" * 70)
    print()

    return result


def demonstrate_jit_compilation():
    """
    Demonstrate JIT compilation for custom functions.

    Shows how to use JIT compilation to optimize custom
    mathematical operations for GPU/CPU.
    """
    print("=" * 70)
    print("JIT COMPILATION EXAMPLE")
    print("=" * 70)

    def compute_moving_average(data, window_size=10):
        """Compute moving average (Note: JIT removed due to dynamic slicing)."""
        # Dynamic slicing isn't compatible with JIT, so we use regular execution
        # For production code, use lax.dynamic_slice for JIT-compatible dynamic slicing
        result = jnp.zeros_like(data)
        for i in range(len(data)):
            start = int(jnp.maximum(0, i - window_size // 2))
            end = int(jnp.minimum(len(data), i + window_size // 2 + 1))
            result = result.at[i].set(jnp.mean(data[start:end]))
        return result

    # Create test data
    data = jnp.array(np.random.randn(10_000))

    # First call: compilation + execution
    print("\nFirst call (includes compilation)...")
    start = time.time()
    compute_moving_average(data, window_size=20)
    compile_time = time.time() - start
    print(f"  Time: {compile_time * 1000:.2f} ms")

    # Second call: cached, much faster
    print("\nSecond call (using cached compilation)...")
    start = time.time()
    compute_moving_average(data, window_size=20)
    cached_time = time.time() - start
    print(f"  Time: {cached_time * 1000:.2f} ms")

    if compile_time > cached_time:
        speedup = compile_time / cached_time
        print(f"\nSpeedup from caching: {speedup:.1f}x")
    print()

    print("üí° Key takeaway:")
    print("   JIT compilation has a one-time cost, but subsequent")
    print("   calls are much faster. Use @jit for repeated operations!")

    print("=" * 70)
    print()


def demonstrate_batch_processing():
    """
    Demonstrate efficient batch processing for GPU.

    Shows how to structure code for maximum GPU efficiency
    by processing multiple datasets in parallel.
    """
    print("=" * 70)
    print("BATCH PROCESSING FOR GPU EFFICIENCY")
    print("=" * 70)

    # Create multiple datasets
    n_datasets = 10
    print(f"\nCreating {n_datasets} datasets...")

    datasets = []
    for i in range(n_datasets):
        x = np.linspace(0, 10, 1000)
        y = np.sin(x * (i + 1) * 0.5) + 0.1 * np.random.normal(size=len(x))
        ds = OneDimensionalDataset(x, y)
        ds.name = f"Dataset {i + 1}"
        datasets.append(ds)

    # Process sequentially
    print("\nProcessing sequentially...")
    pipeline = Pipeline([GaussianSmooth(sigma=2.0)])

    start = time.time()
    [pipeline.apply_to(ds) for ds in datasets]
    sequential_time = time.time() - start

    print(f"  Sequential processing time: {sequential_time * 1000:.2f} ms")

    # For true batch processing, you would stack the data
    # and process in a single operation (requires custom implementation)
    print("\nüí° Batch processing tips:")
    print("   1. Stack multiple datasets into single arrays")
    print("   2. Use vmap/vectorization for parallel processing")
    print("   3. Minimize data transfers between CPU and GPU")
    print("   4. Process largest batches that fit in GPU memory")

    print("=" * 70)
    print()


def memory_management_tips():
    """Print tips for GPU memory management."""
    print("=" * 70)
    print("GPU MEMORY MANAGEMENT TIPS")
    print("=" * 70)
    print()
    print("1. **Monitor memory usage:**")
    print("   JAX manages GPU memory automatically, but you can")
    print("   monitor usage to avoid out-of-memory errors.")
    print()
    print("2. **Clear unused arrays:**")
    print("   Delete large arrays when no longer needed:")
    print("   ```python")
    print("   del large_array")
    print("   ```")
    print()
    print("3. **Use chunking for large datasets:**")
    print("   Process data in chunks if it doesn't fit in GPU memory:")
    print("   ```python")
    print("   chunk_size = 10_000")
    print("   for i in range(0, len(data), chunk_size):")
    print("       chunk = data[i:i+chunk_size]")
    print("       process(chunk)")
    print("   ```")
    print()
    print("4. **Configure JAX memory allocation:**")
    print("   Set environment variable for pre-allocation:")
    print("   ```bash")
    print("   export XLA_PYTHON_CLIENT_PREALLOCATE=false")
    print("   ```")
    print()
    print("=" * 70)
    print()


def main():
    """Run all GPU acceleration demonstrations."""
    print("\n")
    print("‚ïî" + "=" * 68 + "‚ïó")
    print("‚ïë" + " " * 15 + "GPU ACCELERATION EXAMPLE" + " " * 29 + "‚ïë")
    print("‚ïë" + " " * 18 + "quantiq Framework" + " " * 33 + "‚ïë")
    print("‚ïö" + "=" * 68 + "‚ïù")
    print()

    # 1. Device information
    print_device_info()

    # 2. Performance benchmark
    benchmark_cpu_vs_gpu()

    # 3. JIT compilation
    demonstrate_jit_compilation()

    # 4. Batch processing
    demonstrate_batch_processing()

    # 5. Memory management
    memory_management_tips()

    # Summary
    print("=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print()
    print("‚úì Device information retrieved")
    print("‚úì Performance benchmark completed")
    print("‚úì JIT compilation demonstrated")
    print("‚úì Batch processing explained")
    print("‚úì Memory management tips provided")
    print()

    backend = get_backend()
    if backend == "jax":
        device_info = get_device_info()
        platform = device_info.get("platform", "cpu")
        if platform == "gpu":
            print("üöÄ GPU acceleration is ACTIVE!")
            print("   Your code is running at optimal performance.")
        else:
            print("üí° GPU not detected - running on CPU")
            print("   Install GPU-enabled JAX for acceleration:")
            print("   - NVIDIA: pip install 'jax[cuda12]'")
            print("   - Apple: pip install 'jax[metal]'")
            print("   - AMD: pip install 'jax[rocm]'")
    else:
        print("üí° Using NumPy backend")
        print("   Install JAX for automatic GPU acceleration:")
        print("   pip install jax jaxlib")

    print()
    print("=" * 70)
    print("\n‚ú® Example completed successfully!\n")


if __name__ == "__main__":
    main()
